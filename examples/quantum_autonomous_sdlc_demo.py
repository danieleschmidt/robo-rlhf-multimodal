#!/usr/bin/env python3
"""
Quantum Autonomous SDLC Execution Demo

This example demonstrates the complete quantum-inspired autonomous SDLC execution
capabilities, including task planning, multi-objective optimization, predictive
analytics, and autonomous decision-making.

Usage:
    python examples/quantum_autonomous_sdlc_demo.py
"""

import asyncio
import time
import logging
from pathlib import Path
from typing import Dict, Any
import json
import numpy as np

from robo_rlhf.quantum import (
    QuantumTaskPlanner,
    QuantumDecisionEngine,
    QuantumOptimizer,
    MultiObjectiveOptimizer,
    AutonomousSDLCExecutor,
    PredictiveAnalytics,
    ResourcePredictor
)
from robo_rlhf.quantum.planner import TaskPriority
from robo_rlhf.quantum.optimizer import OptimizationObjective, OptimizationConstraint
from robo_rlhf.quantum.autonomous import SDLCPhase, ExecutionContext
from robo_rlhf.quantum.analytics import PredictionType
from robo_rlhf.core import get_logger, setup_logging


class QuantumSDLCDemo:
    """Comprehensive demo of quantum-inspired autonomous SDLC capabilities."""
    
    def __init__(self):
        # Setup logging
        setup_logging(level="INFO", structured=False)
        self.logger = get_logger(__name__)
        
        # Project configuration
        self.project_path = Path(__file__).parent.parent
        self.config = {
            "quantum": {
                "superposition_depth": 3,
                "entanglement_strength": 0.7,
                "collapse_threshold": 0.9,
                "temperature": 1.0,
                "annealing_schedule": "exponential",
                "coherence_time": 100.0
            },
            "planning": {
                "max_parallel_tasks": 4,
                "resource_buffer": 0.2,
                "success_threshold": 0.85
            },
            "optimization": {
                "population_size": 50,
                "elite_size": 10,
                "mutation_rate": 0.1,
                "crossover_rate": 0.8
            },
            "autonomous": {
                "max_parallel": 3,
                "auto_rollback": True,
                "quality_threshold": 0.85,
                "optimization_frequency": 5
            },
            "analytics": {
                "window_size": 50,
                "prediction_horizon": 300.0,
                "retrain_interval": 1800.0,
                "anomaly_threshold": 0.05
            }
        }
        
        # Initialize quantum components
        self.task_planner = QuantumTaskPlanner(self.config)
        self.decision_engine = QuantumDecisionEngine()
        self.optimizer = QuantumOptimizer(self.config)
        self.multi_optimizer = MultiObjectiveOptimizer(self.optimizer)
        self.analytics = PredictiveAnalytics(self.config)
        self.resource_predictor = ResourcePredictor(self.analytics)
        self.autonomous_executor = AutonomousSDLCExecutor(self.project_path, self.config)
        
        self.logger.info("ğŸš€ Quantum SDLC Demo initialized - Ready for autonomous execution!")
    
    async def run_complete_demo(self) -> Dict[str, Any]:
        """Run complete quantum autonomous SDLC demonstration."""
        demo_results = {
            "start_time": time.time(),
            "phases_completed": [],
            "quantum_features_demonstrated": [],
            "performance_metrics": {},
            "optimization_results": {},
            "predictions_made": [],
            "autonomous_decisions": [],
            "overall_success": False
        }
        
        self.logger.info("ğŸŒŸ Starting Complete Quantum Autonomous SDLC Demo")
        
        try:
            # Phase 1: Quantum Task Planning
            self.logger.info("ğŸ“‹ Phase 1: Quantum Task Planning with Superposition")
            planning_results = await self._demonstrate_quantum_planning()
            demo_results["phases_completed"].append("quantum_planning")
            demo_results["quantum_features_demonstrated"].extend([
                "quantum_superposition", "task_entanglement", "solution_collapse"
            ])
            
            # Phase 2: Multi-Objective Optimization
            self.logger.info("ğŸ¯ Phase 2: Multi-Objective Optimization")
            optimization_results = await self._demonstrate_optimization()
            demo_results["optimization_results"] = optimization_results
            demo_results["phases_completed"].append("multi_objective_optimization")
            demo_results["quantum_features_demonstrated"].extend([
                "quantum_annealing", "pareto_optimization", "genetic_algorithms"
            ])
            
            # Phase 3: Predictive Analytics
            self.logger.info("ğŸ”® Phase 3: Predictive Analytics & Machine Learning")
            await self._demonstrate_predictive_analytics()
            demo_results["phases_completed"].append("predictive_analytics")
            demo_results["quantum_features_demonstrated"].extend([
                "anomaly_detection", "pattern_recognition", "resource_forecasting"
            ])
            
            # Phase 4: Autonomous Decision Making
            self.logger.info("ğŸ§  Phase 4: Autonomous Decision Making")
            decision_results = await self._demonstrate_autonomous_decisions()
            demo_results["autonomous_decisions"] = decision_results
            demo_results["phases_completed"].append("autonomous_decisions")
            demo_results["quantum_features_demonstrated"].extend([
                "quantum_decision_engine", "contextual_reasoning", "adaptive_learning"
            ])
            
            # Phase 5: Autonomous SDLC Execution
            self.logger.info("âš¡ Phase 5: Autonomous SDLC Execution")
            execution_results = await self._demonstrate_autonomous_execution()
            demo_results["performance_metrics"] = execution_results
            demo_results["phases_completed"].append("autonomous_execution")
            demo_results["quantum_features_demonstrated"].extend([
                "self_optimization", "failure_recovery", "quality_gates"
            ])
            
            # Phase 6: Real-time Resource Management
            self.logger.info("ğŸ“Š Phase 6: Real-time Resource Management")
            resource_results = await self._demonstrate_resource_management()
            demo_results["phases_completed"].append("resource_management")
            demo_results["quantum_features_demonstrated"].extend([
                "predictive_scaling", "capacity_planning", "cost_optimization"
            ])
            
            # Generate comprehensive report
            demo_results["predictions_made"] = len(self.analytics.prediction_history)
            demo_results["overall_success"] = len(demo_results["phases_completed"]) >= 5
            demo_results["execution_time"] = time.time() - demo_results["start_time"]
            
            # Final analysis
            await self._generate_demo_analysis(demo_results)
            
            self.logger.info(f"âœ… Demo completed successfully in {demo_results['execution_time']:.2f}s")
            
        except Exception as e:
            self.logger.error(f"âŒ Demo failed: {str(e)}")
            demo_results["error"] = str(e)
            demo_results["overall_success"] = False
        
        return demo_results
    
    async def _demonstrate_quantum_planning(self) -> Dict[str, Any]:
        """Demonstrate quantum task planning capabilities."""
        self.logger.info("  ğŸ”¬ Creating quantum task superposition...")
        
        # Define complex SDLC objective
        objective = "Implement autonomous robotics RLHF pipeline with quantum optimization"
        requirements = [
            "multimodal_data_collection",
            "preference_learning",
            "policy_optimization", 
            "deployment_automation",
            "monitoring_system",
            "security_validation",
            "performance_testing"
        ]
        
        # Create quantum plan
        plan = self.task_planner.create_quantum_plan(objective, requirements)
        
        self.logger.info(f"  âœ¨ Generated quantum plan with {len(plan.tasks)} tasks")
        self.logger.info(f"  ğŸ”— Created {len(plan.parallel_groups)} parallel execution groups")
        
        # Execute quantum plan
        execution_results = await self.task_planner.execute_quantum_plan(plan)
        
        success_rate = sum(1 for result in execution_results["task_results"].values() 
                          if result.get("success", False)) / len(execution_results["task_results"])
        
        self.logger.info(f"  âš¡ Quantum execution success rate: {success_rate:.1%}")
        
        return {
            "plan_id": plan.id,
            "tasks_generated": len(plan.tasks),
            "parallel_groups": len(plan.parallel_groups),
            "execution_success_rate": success_rate,
            "collapsed_solutions": len(execution_results.get("collapsed_solutions", {})),
            "quantum_coherence": plan.estimated_completion
        }
    
    async def _demonstrate_optimization(self) -> Dict[str, Any]:
        """Demonstrate multi-objective quantum optimization."""
        self.logger.info("  ğŸ¯ Initializing multi-objective quantum optimizer...")
        
        # Define SDLC optimization problem
        pipeline_config = {
            "build_system": "python",
            "testing_framework": "pytest",
            "deployment_platform": "kubernetes",
            "monitoring_stack": "prometheus"
        }
        
        objectives = [
            OptimizationObjective.MINIMIZE_TIME,
            OptimizationObjective.MAXIMIZE_QUALITY,
            OptimizationObjective.MAXIMIZE_RELIABILITY,
            OptimizationObjective.MINIMIZE_RESOURCES
        ]
        
        constraints = [
            OptimizationConstraint(
                name="test_coverage",
                constraint_type="bound",
                target_value=0.85,
                tolerance=0.05
            ),
            OptimizationConstraint(
                name="cpu_allocation",
                constraint_type="inequality", 
                target_value=0.8,
                tolerance=0.1
            )
        ]
        
        # Run optimization
        solutions = await self.multi_optimizer.optimize_sdlc_pipeline(
            pipeline_config, objectives, constraints
        )
        
        # Analyze results
        if solutions:
            best_solution = solutions[0]
            fitness_scores = [sol.fitness_score for sol in solutions]
            
            self.logger.info(f"  ğŸ† Found {len(solutions)} Pareto-optimal solutions")
            self.logger.info(f"  ğŸ“ˆ Best fitness score: {best_solution.fitness_score:.3f}")
            self.logger.info(f"  ğŸ”¬ Average fitness: {np.mean(fitness_scores):.3f}")
        
        # Get optimization summary
        summary = self.multi_optimizer.get_optimization_summary()
        
        return {
            "solutions_found": len(solutions),
            "best_fitness": solutions[0].fitness_score if solutions else 0.0,
            "average_fitness": np.mean(fitness_scores) if solutions else 0.0,
            "pareto_front_size": len(solutions),
            "optimization_summary": summary,
            "constraints_satisfied": all(sol.constraints_satisfied for sol in solutions[:10])
        }
    
    async def _demonstrate_predictive_analytics(self) -> None:
        """Demonstrate predictive analytics capabilities."""
        self.logger.info("  ğŸ“Š Generating synthetic metric data...")
        
        # Generate realistic metric data
        await self._generate_synthetic_metrics()
        
        # Demonstrate predictions
        predictions = []
        for prediction_type in PredictionType:
            if prediction_type == PredictionType.RESOURCE_USAGE:
                result = await self.analytics.predict(prediction_type, "cpu_usage")
                predictions.append(result)
                self.logger.info(f"  ğŸ”® CPU usage prediction: {result.predicted_value:.2f} (confidence: {result.confidence:.1%})")
            
            elif prediction_type == PredictionType.PERFORMANCE:
                result = await self.analytics.predict(prediction_type, "execution_time")
                predictions.append(result)
                self.logger.info(f"  âš¡ Performance prediction: {result.predicted_value:.1f}s (confidence: {result.confidence:.1%})")
        
        # Anomaly detection
        anomalies = await self.analytics.detect_anomalies("cpu_usage")
        if anomalies:
            self.logger.info(f"  ğŸš¨ Detected {len(anomalies)} anomalies in CPU usage")
        
        # Pattern identification
        patterns = await self.analytics.identify_patterns("memory_usage")
        if patterns["clusters"] > 0:
            self.logger.info(f"  ğŸ” Identified {patterns['clusters']} usage patterns")
        
        # Generate insights
        insights = await self.analytics.generate_insights()
        self.logger.info(f"  ğŸ’¡ Generated {len(insights)} actionable insights")
        
        for insight in insights[:3]:  # Show top 3 insights
            self.logger.info(f"     â€¢ {insight['description']}")
    
    async def _demonstrate_autonomous_decisions(self) -> List[Dict[str, Any]]:
        """Demonstrate autonomous decision making."""
        self.logger.info("  ğŸ¤– Testing autonomous decision engine...")
        
        decisions = []
        
        # Decision scenario 1: Deployment strategy selection
        context = {
            "current_load": 0.7,
            "error_rate": 0.02,
            "available_resources": {"cpu": 0.6, "memory": 0.8},
            "environmental_factors": {"time_of_day": 14, "day_of_week": 2}
        }
        
        options = [
            {
                "strategy": "blue_green",
                "risk": 0.1,
                "rollback_time": 30,
                "resource_cost": 0.8,
                "type": "deployment"
            },
            {
                "strategy": "rolling_update",
                "risk": 0.3,
                "rollback_time": 120,
                "resource_cost": 0.4,
                "type": "deployment"
            },
            {
                "strategy": "canary",
                "risk": 0.2,
                "rollback_time": 60,
                "resource_cost": 0.6,
                "type": "deployment"
            }
        ]
        
        criteria = {
            "risk": -1.0,  # Minimize risk
            "rollback_time": -0.5,  # Minimize rollback time
            "resource_cost": -0.3  # Minimize resource cost
        }
        
        decision = await self.decision_engine.make_autonomous_decision(context, options, criteria)
        decisions.append(decision)
        
        selected_strategy = decision["selected_option"]["strategy"]
        confidence = decision["confidence"]
        
        self.logger.info(f"  ğŸ¯ Selected deployment strategy: {selected_strategy} (confidence: {confidence:.1%})")
        
        # Decision scenario 2: Resource scaling decision
        scaling_context = {
            "cpu_usage": 0.85,
            "memory_usage": 0.75,
            "request_rate": 1000,
            "environmental_factors": {"peak_hours": True, "scaling_history": 0.8}
        }
        
        scaling_options = [
            {
                "action": "scale_up",
                "instances": 2,
                "cost": 100,
                "response_time_improvement": 0.4,
                "type": "scaling"
            },
            {
                "action": "optimize_existing",
                "instances": 0,
                "cost": 0,
                "response_time_improvement": 0.2,
                "type": "scaling"
            },
            {
                "action": "scale_horizontally",
                "instances": 4,
                "cost": 200,
                "response_time_improvement": 0.6,
                "type": "scaling"
            }
        ]
        
        scaling_criteria = {
            "cost": -0.4,
            "response_time_improvement": 1.0,
            "instances": -0.2
        }
        
        scaling_decision = await self.decision_engine.make_autonomous_decision(
            scaling_context, scaling_options, scaling_criteria
        )
        decisions.append(scaling_decision)
        
        selected_action = scaling_decision["selected_option"]["action"]
        scaling_confidence = scaling_decision["confidence"]
        
        self.logger.info(f"  ğŸ“ˆ Selected scaling action: {selected_action} (confidence: {scaling_confidence:.1%})")
        
        # Get decision analytics
        analytics = self.decision_engine.get_decision_analytics()
        self.logger.info(f"  ğŸ“Š Decision engine analytics: {analytics['total_decisions']} decisions, avg confidence: {analytics['average_confidence']:.1%}")
        
        return decisions
    
    async def _demonstrate_autonomous_execution(self) -> Dict[str, Any]:
        """Demonstrate autonomous SDLC execution."""
        self.logger.info("  ğŸ”„ Initiating autonomous SDLC execution...")
        
        # Create execution context
        context = ExecutionContext(
            project_path=self.project_path,
            environment="demo",
            configuration=self.config,
            resource_limits={"cpu": 1.0, "memory": 2.0, "storage": 10.0},
            quality_gates={"test_coverage": 0.8, "success_rate": 0.9},
            monitoring_config={"enabled": True, "interval": 30}
        )
        
        # Select phases for demo (limited set for speed)
        target_phases = [
            SDLCPhase.ANALYSIS,
            SDLCPhase.TESTING,
            SDLCPhase.INTEGRATION
        ]
        
        # Execute autonomous SDLC
        execution_summary = await self.autonomous_executor.execute_autonomous_sdlc(
            target_phases, context
        )
        
        success_rate = execution_summary["successful_actions"] / max(1, execution_summary["total_actions"])
        
        self.logger.info(f"  âœ… Autonomous execution completed: {execution_summary['total_actions']} actions")
        self.logger.info(f"  ğŸ“Š Success rate: {success_rate:.1%}")
        self.logger.info(f"  ğŸ”§ Optimizations applied: {execution_summary['optimizations_applied']}")
        self.logger.info(f"  ğŸ¯ Quality score: {execution_summary['quality_score']:.2f}")
        
        # Get execution summary
        detailed_summary = self.autonomous_executor.get_execution_summary()
        
        return {
            **execution_summary,
            "detailed_metrics": detailed_summary
        }
    
    async def _demonstrate_resource_management(self) -> Dict[str, Any]:
        """Demonstrate real-time resource management."""
        self.logger.info("  ğŸ’¾ Demonstrating predictive resource management...")
        
        # Generate resource usage data
        await self._generate_resource_metrics()
        
        # Predict resource demand
        predictions = await self.resource_predictor.predict_resource_demand(time_horizon=1800.0)
        
        for resource_type, prediction in predictions.items():
            self.logger.info(f"  ğŸ“ˆ {resource_type.upper()}: current={prediction.current_usage:.1%}, "
                           f"predicted={prediction.predicted_usage:.1%}, confidence={prediction.confidence:.1%}")
            
            if prediction.recommendations:
                self.logger.info(f"     ğŸ’¡ Recommendations: {prediction.recommendations[0]}")
        
        # Optimize resource allocation
        current_allocation = {"cpu": 0.6, "memory": 0.7, "storage": 0.4, "network": 0.5}
        optimized_allocation = await self.resource_predictor.optimize_resource_allocation(
            current_allocation, predictions
        )
        
        # Calculate health score
        health_score = self.resource_predictor.get_resource_health_score(predictions)
        self.logger.info(f"  ğŸ’š Resource health score: {health_score:.1%}")
        
        # Generate capacity plan
        capacity_plan = self.resource_predictor.generate_capacity_plan(predictions)
        self.logger.info(f"  ğŸ“‹ Capacity plan: {len(capacity_plan['recommendations'])} recommendations, urgency={capacity_plan['urgency']}")
        
        return {
            "predictions": {k: {
                "current": v.current_usage,
                "predicted": v.predicted_usage,
                "confidence": v.confidence
            } for k, v in predictions.items()},
            "optimized_allocation": optimized_allocation,
            "health_score": health_score,
            "capacity_plan": capacity_plan
        }
    
    async def _generate_synthetic_metrics(self) -> None:
        """Generate realistic synthetic metrics for demo."""
        base_time = time.time() - 3600  # Start 1 hour ago
        
        for i in range(50):
            timestamp = base_time + i * 60  # Every minute
            
            # Generate realistic CPU usage with patterns
            hour_factor = np.sin(2 * np.pi * i / 60) * 0.2 + 0.5
            noise = np.random.normal(0, 0.1)
            cpu_usage = max(0.1, min(0.95, hour_factor + noise))
            
            # Memory usage correlated with CPU
            memory_usage = cpu_usage * 0.8 + np.random.normal(0, 0.05)
            memory_usage = max(0.1, min(0.9, memory_usage))
            
            # Execution time inversely related to resources
            execution_time = 100 / (cpu_usage + 0.1) + np.random.normal(0, 5)
            execution_time = max(10, execution_time)
            
            metrics = {
                "cpu_usage": cpu_usage,
                "memory_usage": memory_usage,
                "execution_time": execution_time,
                "network_io": np.random.uniform(0.1, 0.8),
                "disk_io": np.random.uniform(0.1, 0.6)
            }
            
            await self.analytics.ingest_metrics(metrics, "synthetic")
            
            # Add some anomalies
            if i in [15, 35]:  # Anomalies at specific points
                anomaly_metrics = {
                    "cpu_usage": 0.95 + np.random.uniform(0, 0.05),
                    "memory_usage": 0.98,
                    "execution_time": 300
                }
                await self.analytics.ingest_metrics(anomaly_metrics, "anomaly_injection")
    
    async def _generate_resource_metrics(self) -> None:
        """Generate resource usage metrics."""
        resources = ["cpu", "memory", "storage", "network", "gpu"]
        base_time = time.time() - 1800  # Start 30 minutes ago
        
        for i in range(30):
            timestamp = base_time + i * 60
            
            metrics = {}
            for resource in resources:
                # Different patterns for different resources
                if resource == "cpu":
                    base_usage = 0.4 + 0.3 * np.sin(2 * np.pi * i / 20)
                elif resource == "memory":
                    base_usage = 0.6 + 0.1 * np.sin(2 * np.pi * i / 15)
                elif resource == "storage":
                    base_usage = 0.3 + i * 0.01  # Gradually increasing
                elif resource == "network":
                    base_usage = 0.2 + 0.4 * np.random.random()  # More variable
                else:  # gpu
                    base_usage = 0.7 + 0.2 * np.sin(2 * np.pi * i / 10)
                
                usage = max(0.1, min(0.95, base_usage + np.random.normal(0, 0.05)))
                metrics[f"{resource}_usage"] = usage
                metrics[f"{resource}_peak"] = usage * 1.2
            
            await self.analytics.ingest_metrics(metrics, "resource_monitor")
    
    async def _generate_demo_analysis(self, results: Dict[str, Any]) -> None:
        """Generate comprehensive demo analysis."""
        self.logger.info("ğŸ“ˆ Generating comprehensive analysis...")
        
        # Feature coverage analysis
        all_quantum_features = [
            "quantum_superposition", "task_entanglement", "solution_collapse",
            "quantum_annealing", "pareto_optimization", "genetic_algorithms",
            "anomaly_detection", "pattern_recognition", "resource_forecasting",
            "quantum_decision_engine", "contextual_reasoning", "adaptive_learning",
            "self_optimization", "failure_recovery", "quality_gates",
            "predictive_scaling", "capacity_planning", "cost_optimization"
        ]
        
        feature_coverage = len(results["quantum_features_demonstrated"]) / len(all_quantum_features)
        
        self.logger.info(f"  ğŸŒŸ Quantum Feature Coverage: {feature_coverage:.1%}")
        self.logger.info(f"  ğŸ“Š Phases Completed: {len(results['phases_completed'])}/6")
        self.logger.info(f"  ğŸ”® Predictions Made: {results['predictions_made']}")
        self.logger.info(f"  ğŸ¤– Autonomous Decisions: {len(results['autonomous_decisions'])}")
        
        # Performance analysis
        if "performance_metrics" in results and results["performance_metrics"]:
            perf = results["performance_metrics"]
            self.logger.info(f"  âš¡ Execution Performance:")
            self.logger.info(f"     â€¢ Total Actions: {perf.get('total_actions', 0)}")
            self.logger.info(f"     â€¢ Success Rate: {perf.get('successful_actions', 0) / max(1, perf.get('total_actions', 1)):.1%}")
            self.logger.info(f"     â€¢ Quality Score: {perf.get('quality_score', 0):.2f}")
        
        # Save results to file
        results_file = self.project_path / "quantum_demo_results.json"
        with open(results_file, 'w') as f:
            # Convert numpy types for JSON serialization
            json_results = self._convert_for_json(results)
            json.dump(json_results, f, indent=2)
        
        self.logger.info(f"  ğŸ’¾ Results saved to: {results_file}")
    
    def _convert_for_json(self, obj):
        """Convert numpy types for JSON serialization."""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._convert_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_for_json(item) for item in obj]
        else:
            return obj


async def main():
    """Main demo entry point."""
    print("ğŸš€ Quantum Autonomous SDLC Execution Demo")
    print("=" * 50)
    print("This demo showcases quantum-inspired autonomous capabilities:")
    print("â€¢ Quantum Task Planning with Superposition")
    print("â€¢ Multi-Objective Optimization")
    print("â€¢ Predictive Analytics & ML")
    print("â€¢ Autonomous Decision Making")
    print("â€¢ Self-Optimizing SDLC Execution")
    print("â€¢ Real-time Resource Management")
    print("=" * 50)
    
    demo = QuantumSDLCDemo()
    
    try:
        results = await demo.run_complete_demo()
        
        print("\nğŸ‰ Demo Results Summary:")
        print(f"âœ… Overall Success: {results['overall_success']}")
        print(f"â±ï¸  Total Time: {results['execution_time']:.2f}s")
        print(f"ğŸ“Š Phases Completed: {len(results['phases_completed'])}")
        print(f"ğŸŒŸ Quantum Features: {len(results['quantum_features_demonstrated'])}")
        print(f"ğŸ”® Predictions: {results['predictions_made']}")
        print(f"ğŸ¤– Decisions: {len(results['autonomous_decisions'])}")
        
        if results['overall_success']:
            print("\nğŸŠ Quantum Autonomous SDLC Demo completed successfully!")
            print("All quantum-inspired capabilities have been demonstrated.")
        else:
            print("\nâš ï¸  Demo completed with some limitations.")
            if "error" in results:
                print(f"Error: {results['error']}")
        
        print(f"\nğŸ“„ Detailed results saved to: quantum_demo_results.json")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ Demo interrupted by user")
    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Run the demo
    asyncio.run(main())